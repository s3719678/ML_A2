{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff64c064",
   "metadata": {},
   "source": [
    "### List of what you need to do:\n",
    "- ~~Define the Queue (generic)~~\n",
    "- ~~Define the Q-Table (just empty table with all possible states)~~\n",
    "- Start Q-Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1e3ed3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.5.3)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.5.15)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (8.1.0)\n"
     ]
    }
   ],
   "source": [
    "# All Imports and Constants\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import copy\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 500\n",
    "\n",
    "# parameters for epsilon decay policy\n",
    "EPSILON = 1.0 # not a constant, going to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "epsilon_decay_value = EPSILON / (END_EPSILON_DECAYING - START_EPSILON_DECAYING) # 1 / (250 - 1) = 1 / 249\n",
    "\n",
    "# for testing\n",
    "N_TEST_RUNS = 100\n",
    "TEST_INTERVAL = 50\n",
    "\n",
    "MAX_TIMESLOTS = 100\n",
    "MAX_WAIT_STATE = 50 #used as upper limit in q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20ad9c",
   "metadata": {},
   "source": [
    "# Generic Queue Simulator\n",
    "We want to define a generic queue simulator that has its own arrival rates/mean delay requirements so that we can use it for PQ1, PQ2 and Best-Effort, as well as the other queues such as FIFO, RR, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5e88a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueSimulator(gym.Env):\n",
    "    def __init__(self, arrival_rates, mean_delay_requirements, queues_finished_timeslots):\n",
    "        super(QueueSimulator, self).__init__()\n",
    "        self.arrival_rates = arrival_rates\n",
    "        self.mean_delay_requirements = mean_delay_requirements\n",
    "        self.current_timeslot = 1\n",
    "        self.queues_finished_timeslots = queues_finished_timeslots # Need some reference to this so that I can use it again after reset\n",
    "        self.queues = copy.deepcopy(queues_finished_timeslots)\n",
    "        \n",
    "        # Action Space is 3, because we have 3 queues to choose from\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # We know the observation space are the range of possible and observable values. This is wait times,\n",
    "        # so wait times can be 0 or infinity technically.\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0]), high=np.array([np.inf, np.inf, np.inf]), dtype=np.dtype(int))\n",
    "   \n",
    "    # Get current wait times for each queue (so how long a queue has been waiting for on a packet to send)\n",
    "    def calc_state(self):\n",
    "        calc_state = [0, 0, 0]\n",
    "        \n",
    "        for i, queue in enumerate(self.queues):\n",
    "            queue_total_wait = 0\n",
    "            for packet in queue:\n",
    "                if packet <= self.current_timeslot:\n",
    "                    queue_total_wait += (self.current_timeslot - packet)\n",
    "            calc_state[i] = queue_total_wait\n",
    "            \n",
    "        return calc_state\n",
    "\n",
    "    # Every step means you calc state_new, reward, done and info\n",
    "    # Reward should be based on wait time for a packet, as in give HIGHEST reward when \n",
    "    #     queue has had to wait for its entire mean_delay_requirement duration\n",
    "    def step(self, action):\n",
    "        # First, check how long each queue has been waiting for (this is the initial state)\n",
    "        current_state = self.calc_state()\n",
    "        \n",
    "        # Now calc reward and delete front package from queue[action]\n",
    "        reward = 0.0\n",
    "        if current_state[action]/self.mean_delay_requirements[action] >= 1.0:\n",
    "            reward = 100.0\n",
    "        else:\n",
    "            # Not really sure if 10 is necessary here, probs arbitrary\n",
    "            reward = (10 / self.mean_delay_requirements[action]) * (current_state[action])\n",
    "            \n",
    "        if (len(self.queues[action]) > 0):\n",
    "            del self.queues[action][0]\n",
    "        \n",
    "        # Now get new state to send back\n",
    "        new_state = self.calc_state()\n",
    "        \n",
    "        # done = True only when all queues have no packets left\n",
    "        done = False\n",
    "        if all(len(queue) == 0 for queue in self.queues):\n",
    "            done = True\n",
    "\n",
    "        self.current_timeslot += 1\n",
    "        return new_state, reward, done, {}\n",
    "        # TODO Impl\n",
    "        \n",
    "    # Need to reset the vars between episodes like timeslots\n",
    "    # Since q_learning expects state reset too, return the calc_state method\n",
    "    # Need to add packets back into queue because i deleted them every run lol\n",
    "    def reset(self):\n",
    "        self.timeslots = 0\n",
    "        self.queues = copy.deepcopy(self.queues_finished_timeslots)\n",
    "        return self.calc_state()\n",
    "        \n",
    "    def render(self):\n",
    "        return self\n",
    "        # TODO Impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a769d35c",
   "metadata": {},
   "source": [
    "# Pre-populating Queues\n",
    "If given the arrival_rates and mean_delay_requirements, you could calculate what timeslots a packet will arrive for any number of timeslots. Hence, we believe that you should just 'pre-populate' your queues with the times that packets arrive at, since this simplifies all of the packet arrival/transmission, and you can later use this to measure wait times and give this information to your model to determine an action to take every step of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f79e7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 11, 14, 17, 21, 24, 27, 31, 34, 37, 41, 44, 47, 51, 54, 57, 61, 64, 67, 71, 74, 77, 81, 84, 87, 91, 94, 97] -> Length = 29\n",
      "[4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100] -> Length = 25\n",
      "[3, 5, 8, 10, 13, 15, 18, 20, 23, 25, 28, 30, 33, 35, 38, 40, 43, 45, 48, 50, 53, 55, 58, 60, 63, 65, 68, 70, 73, 75, 78, 80, 83, 85, 88, 90, 93, 95, 98, 100] -> Length = 40\n"
     ]
    }
   ],
   "source": [
    "# Create the basic env and put logic for the actions\n",
    "arrival_rates = [0.3, 0.25, 0.4]\n",
    "mean_delay_requirements = [6, 4, np.inf]\n",
    "# Keep track of current packets by using another array, which has index corresponding to the arrival_rates\n",
    "queues_packet_status = [0, 0, 0]\n",
    "# See all timeslots where a queue finished transmitting a packet\n",
    "queues_finished_timeslots = [[], [], []]\n",
    "\n",
    "# At each time interval, increment each queue's current packet status by the arrival rate amount\n",
    "# if packet status >= 1, get the extra amt above 1 and change packet status to just that\n",
    "for timeslot in range(1, MAX_TIMESLOTS+1):\n",
    "    for current_queue in range (len(arrival_rates)):\n",
    "        queues_packet_status[current_queue] += arrival_rates[current_queue]\n",
    "        \n",
    "        if queues_packet_status[current_queue] >= 1.0:\n",
    "            queues_finished_timeslots[current_queue].append(timeslot)\n",
    "            queues_packet_status[current_queue] -= 1.0\n",
    "\n",
    "# Result of queues_finished_timeslots\n",
    "print('{0} -> Length = {1}'.format(queues_finished_timeslots[0], len(queues_finished_timeslots[0])))\n",
    "print('{0} -> Length = {1}'.format(queues_finished_timeslots[1], len(queues_finished_timeslots[1])))\n",
    "print('{0} -> Length = {1}'.format(queues_finished_timeslots[2], len(queues_finished_timeslots[2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2dfd1d",
   "metadata": {},
   "source": [
    "'queues_finished_timeslots' is essentially our environment. We want to take this environment and apply it to a Q-Table, where the Q-Table represents all possible states (states being how long a queue has waited), then some reward for an action given a specific state. Below is what the Q-Table could look like, with a state e.g. (1, 0, 3) saying that the first packet in queue 1 has waited 1 timeslots, queue 2 has waited 0 timeslots, and queue 3 has waited 3 timeslots.\n",
    "\n",
    "|                       | 0           | 1           | 2           |\n",
    "|-----------------------|:-----------:|-------------|-------------|\n",
    "| State (0, 0, 0)       | some-reward | some-reward | some-reward |\n",
    "| State (0, 0, 1)       | some-reward | some-reward | some-reward |\n",
    "| State (0, 1, 1)       | \"         \" | \"         \" | \"         \" |\n",
    "| State (  ...  )       |             |             |             |\n",
    "| State (Inf, Inf, Inf) | some-reward | some-reward | some-reward |\n",
    "\n",
    "# Setting Up Q-Learning\n",
    "First, we need to make a Q-Table with empty values. Access to an entry is given with the state (x, y, z).\n",
    "Also, the upper state cannot be infinity so we can choose an arbitrary value (but still one that is somewhat realistic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "10b26414",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_q_table():\n",
    "    q_table = {}\n",
    "    for q1 in range (MAX_WAIT_STATE+1):\n",
    "        for q2 in range (MAX_WAIT_STATE+1):\n",
    "            for q3 in range (MAX_WAIT_STATE+1):\n",
    "                q_table[q1, q2, q3] = np.zeros(3)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2fe08",
   "metadata": {},
   "source": [
    "# Start Q-Learning Loop\n",
    "1. For every episode, do another 'until done' loop\n",
    "2. While not done:\n",
    "    - Get a random chance, and either get value from Q-Table (exploit) or do random action (explore)\n",
    "    - Do next step() for env\n",
    "    - Update Q-Table and any other variables\n",
    "3. Update epsilon\n",
    "4. Can do some update per episode, but if we're doing minimum 500 episodes more likely to do some performance check every X interval e.g. every 50 episodes get some check in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "adaef181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "Finished 50 episodes\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "Finished 100 episodes\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "Finished 150 episodes\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "Finished 200 episodes\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "0.9959839357429718\n",
      "Finished 250 episodes\n",
      "Finished 300 episodes\n",
      "Finished 350 episodes\n",
      "Finished 400 episodes\n",
      "Finished 450 episodes\n",
      "Finished all episodes\n"
     ]
    }
   ],
   "source": [
    "def max_limit_state(state):\n",
    "    for i, wait_time in enumerate(state):\n",
    "        if wait_time > MAX_WAIT_STATE:\n",
    "            state[i] = MAX_WAIT_STATE\n",
    "    return state\n",
    "\n",
    "def q_learning(env, q_table):\n",
    "    for episode in range(EPISODES):\n",
    "        # Reset all variables per episode\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        epsilon = EPSILON\n",
    "        steps = 0\n",
    "\n",
    "        # Either do action from QTable or random action\n",
    "        while not done:\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(q_table[state])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            # Get the next state, reward, new done value, and info (not sure what this is)\n",
    "            # Also, in early episodes the queues can wait for very long times. Put a limit on the wait times in new_state\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_state = max_limit_state(new_state)\n",
    "#             if episode < 50:\n",
    "#                 print('Epsilon: {0}'.format(epsilon))\n",
    "\n",
    "            # Update QTable and calc reward. Not sure why current_q in example is 'discretState+(action,)'\n",
    "            # Note: Need to convert new_state to tuple since q_table entries are as tuples\n",
    "            q_table_key = tuple(new_state)\n",
    "            new_max_q = np.max(q_table[q_table_key])\n",
    "            current_q = q_table[q_table_key][action]\n",
    "            q_table[q_table_key][action] = (1 - LEARNING_RATE)*current_q + LEARNING_RATE*(reward + DISCOUNT*new_max_q)\n",
    "\n",
    "            state = new_state\n",
    "            steps += 1\n",
    "\n",
    "        # Finished done loop, update epsilon\n",
    "        if END_EPSILON_DECAYING >= episode and episode >= START_EPSILON_DECAYING:\n",
    "            epsilon -= epsilon_decay_value\n",
    "            print(epsilon)\n",
    "\n",
    "        # Print progress every X episodes\n",
    "        if episode % TEST_INTERVAL == 0 and episode != 0:\n",
    "            print('Finished {0} episodes'.format(episode))\n",
    "\n",
    "    print('Finished all episodes')\n",
    "    env.close()\n",
    "    return q_table\n",
    "\n",
    "env = QueueSimulator(arrival_rates, mean_delay_requirements, queues_finished_timeslots)\n",
    "q_table = create_q_table()\n",
    "result_q_table = q_learning(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "329cc58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04489937 0.74376872 0.17066681]\n"
     ]
    }
   ],
   "source": [
    "print(result_q_table[(0, 0, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41827d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
