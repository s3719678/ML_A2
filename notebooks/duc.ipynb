{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "322b2937",
   "metadata": {},
   "source": [
    "### List of what you need to do:\n",
    "- ~~Define the Queue (generic)~~\n",
    "- ~~Define the Q-Table (just empty table with all possible states)~~\n",
    "- Start Q-Learning Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99ff79",
   "metadata": {},
   "source": [
    "# Generic Queue Simulator\n",
    "We want to define a generic queue simulator that has its own arrival rates/mean delay requirements so that we can use it for PQ1, PQ2 and Best-Effort, as well as the other queues such as FIFO, RR, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a45e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.18.3.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.18.5)\n",
      "Collecting pyglet<=1.5.15,>=1.4.0\n",
      "  Downloading pyglet-1.5.15-py3-none-any.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 68.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow<=8.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (8.1.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym) (1.6.0)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.18.3-py3-none-any.whl size=1657518 sha256=2d2fe19cb7d4705a3a1e969545e9d7a0a096d0f19c225d6895a6fd9879f3638a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/1f/63/7a/4368e4c3aedd396d4ab8e9b7922af06433994ebe739853ae4a\n",
      "Successfully built gym\n",
      "Installing collected packages: pyglet, gym\n",
      "Successfully installed gym-0.18.3 pyglet-1.5.15\n"
     ]
    }
   ],
   "source": [
    "# All Imports and Constants\n",
    "import sys\n",
    "!{sys.executable} -m pip install gym\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 500\n",
    "\n",
    "# parameters for epsilon decay policy\n",
    "EPSILON = 1 # not a constant, going to be decayed\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "epsilon_decay_value = EPSILON / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "# for testing\n",
    "N_TEST_RUNS = 100\n",
    "TEST_INTERVAL = 50\n",
    "\n",
    "MAX_TIMESLOTS = 100\n",
    "MAX_WAIT_STATE = 50 #used as upper limit in q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "033054ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueSimulator(gym.Env):\n",
    "    def __init__(self, arrival_rates, mean_delay_requirements, queues_arrival_times):\n",
    "        super(QueueSimulator, self).__init__()\n",
    "        self.arrival_rates = arrival_rates\n",
    "        self.mean_delay_requirements = mean_delay_requirements\n",
    "        self.current_timeslot = 0\n",
    "        self.queues = queues_arrival_times # Basic case of PQ1, PQ2 and Best-Effort\n",
    "        \n",
    "        # Action Space is 3, because we have 3 queues to choose from\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # We know the observation space are the range of possible and observable values. This is wait times,\n",
    "        # so wait times can be 0 or infinity technically.\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0]), high=np.array([np.inf, np.inf, np.inf]), dtype=np.dtype(int))\n",
    "        \n",
    "    def step(self):\n",
    "        return self\n",
    "        # TODO Impl\n",
    "        \n",
    "    def reset(self):\n",
    "        return self\n",
    "        # TODO Impl\n",
    "        \n",
    "    def render(self):\n",
    "        return self\n",
    "        # TODO Impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea74c46",
   "metadata": {},
   "source": [
    "# Pre-populating Queues\n",
    "If given the arrival_rates and mean_delay_requirements, you could calculate what timeslots a packet will arrive for any number of timeslots. Hence, we believe that you should just 'pre-populate' your queues with the times that packets arrive at, since this simplifies all of the packet arrival/transmission, and you can later use this to measure wait times and give this information to your model to determine an action to take every step of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0268ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 10, 13, 16, 20, 23, 26, 30, 33, 36, 40, 43, 46, 50, 53, 56, 60, 63, 66, 70, 73, 76, 80, 83, 86, 90, 93, 96] -> Length = 29\n",
      "[3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99] -> Length = 25\n",
      "[2, 4, 7, 9, 12, 14, 17, 19, 22, 24, 27, 29, 32, 34, 37, 39, 42, 44, 47, 49, 52, 54, 57, 59, 62, 64, 67, 69, 72, 74, 77, 79, 82, 84, 87, 89, 92, 94, 97, 99] -> Length = 40\n"
     ]
    }
   ],
   "source": [
    "# Create the basic env and put logic for the actions\n",
    "arrival_rates = [0.3, 0.25, 0.4]\n",
    "mean_delay_requirements = [6, 4, np.inf]\n",
    "# Keep track of current packets by using another array, which has index corresponding to the arrival_rates\n",
    "queues_packet_status = [0, 0, 0]\n",
    "# See all timeslots where a queue finished transmitting a packet\n",
    "queues_finished_timeslots = [[], [], []]\n",
    "\n",
    "# At each time interval, increment each queue's current packet status by the arrival rate amount\n",
    "# if packet status >= 1, get the extra amt above 1 and change packet status to just that\n",
    "for timeslot in range(MAX_TIMESLOTS):\n",
    "    for current_queue in range (len(arrival_rates)):\n",
    "        queues_packet_status[current_queue] += arrival_rates[current_queue]\n",
    "        \n",
    "        if queues_packet_status[current_queue] >= 1.0:\n",
    "            queues_finished_timeslots[current_queue].append(timeslot)\n",
    "            queues_packet_status[current_queue] -= 1.0\n",
    "\n",
    "# Result of queues_finished_timeslots\n",
    "print('{0} -> Length = {1}'.format(queues_finished_timeslots[0], len(queues_finished_timeslots[0])))\n",
    "print('{0} -> Length = {1}'.format(queues_finished_timeslots[1], len(queues_finished_timeslots[1])))\n",
    "print('{0} -> Length = {1}'.format(queues_finished_timeslots[2], len(queues_finished_timeslots[2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cb408",
   "metadata": {},
   "source": [
    "'queues_finished_timeslots' is essentially our environment. We want to take this environment and apply it to a Q-Table, where the Q-Table represents all possible states (states being how long a queue has waited), then some reward for an action given a specific state. Below is what the Q-Table could look like, with a state e.g. (1, 0, 3) saying that the first packet in queue 1 has waited 1 timeslots, queue 2 has waited 0 timeslots, and queue 3 has waited 3 timeslots.\n",
    "\n",
    "|                       | 0           | 1           | 2           |\n",
    "|-----------------------|:-----------:|-------------|-------------|\n",
    "| State (0, 0, 0)       | some-reward | some-reward | some-reward |\n",
    "| State (0, 0, 1)       | some-reward | some-reward | some-reward |\n",
    "| State (0, 1, 1)       | \"         \" | \"         \" | \"         \" |\n",
    "| State (  ...  )       |             |             |             |\n",
    "| State (Inf, Inf, Inf) | some-reward | some-reward | some-reward |\n",
    "\n",
    "# Setting Up Q-Learning\n",
    "First, we need to make a Q-Table with empty values. Access to an entry is given with the state (x, y, z).\n",
    "Also, the upper state cannot be infinity so we can choose an arbitrary value (but still one that is somewhat realistic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e5448a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table():\n",
    "    q_table = {}\n",
    "    for q1 in range (MAX_WAIT_STATE):\n",
    "        for q2 in range (MAX_WAIT_STATE):\n",
    "            for q3 in range (MAX_WAIT_STATE):\n",
    "                q_table[(q1, q2, q3)] = np.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d26b7a",
   "metadata": {},
   "source": [
    "# Start Q-Learning Loop\n",
    "1. For every episode, do another 'until done' loop\n",
    "2. While not done:\n",
    "    - Get a random chance, and either get value from Q-Table (exploit) or do random action (explore)\n",
    "    - Do next step() for env\n",
    "    - Update Q-Table and any other variables\n",
    "3. Update epsilon\n",
    "4. Can do some update per episode, but if we're doing minimum 500 episodes more likely to do some performance check every X interval e.g. every 50 episodes get some check in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d75adc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "step() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1cd26113069d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mresult_q_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-1cd26113069d>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, q_table)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Get the next state, reward, new done value, and info (not sure what this is)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mstate_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# No need to discretize state_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: step() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "env = QueueSimulator(arrival_rates, mean_delay_requirements, queues_finished_timeslots)\n",
    "q_table = create_q_table()\n",
    "def q_learning(env, q_table):\n",
    "    for episode in range(EPISODES):\n",
    "        # Reset all variables per episode\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        epsilon = EPSILON\n",
    "        steps = 0\n",
    "        \n",
    "        # Either do action from QTable or random action\n",
    "        while not done:\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(q_table[state])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            # Get the next state, reward, new done value, and info (not sure what this is)\n",
    "            state_new, reward, done, info = env.step(action)\n",
    "            # No need to discretize state_new\n",
    "\n",
    "            # Update QTable and calc reward. Not sure why current_q in example is 'discretState+(action,)'\n",
    "            new_max_q = np.max(q_table[state_new])\n",
    "            current_q = q_table[state_new][action]\n",
    "            q_table[state_new][action] = (1 - LEARNING_RATE)*current_q + LEARNING_RATE*(reward + DISCOUNT*new_max_q)\n",
    "\n",
    "            state = state_new\n",
    "            steps += 1\n",
    "        \n",
    "        # Finished done loop, update epsilon\n",
    "        if END_EPSILON_DECAYING >= episode and episode >= START_EPSILON_DECAYING:\n",
    "            epsilon -= epsilon_decay_value\n",
    "            \n",
    "        # Print progress every X episodes\n",
    "        if episode % TEST_INTERVAL == 0:\n",
    "            success_run_ = list()\n",
    "            steps_ = list()\n",
    "            for i in range(N_TEST_RUNS):\n",
    "                success_run, steps = test_model(QTable)\n",
    "                success_run_.append(success_run)\n",
    "                steps_.append(steps)\n",
    "                \n",
    "            print('Testing at Episode {}:'.format(episode))\n",
    "            print('\\t Successful Runs: {}/{}'.format(np.sum(success_run_), N_TEST_RUNS) )\n",
    "            print('\\t Average Steps: {}'.format(np.mean(steps_)))\n",
    "        \n",
    "    print('Finished all episodes')\n",
    "    env.close()\n",
    "    return q_table\n",
    "    \n",
    "result_q_table = q_learning(env, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe2a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
